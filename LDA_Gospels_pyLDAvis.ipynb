{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "## 1 https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0 (Testa esse aqui tbm)\n",
    "##Auctor: Shashank Kapadia\n",
    "\n",
    "## 2 https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html \n",
    "\n",
    "##3 https://sigmoidal.ai/como-criar-uma-wordcloud-em-python/  -- WorldClouds ## \n",
    "##4 http://tirandolicoesdetudo.com.br/criando-uma-nuvem-de-palavras-wordcloud-com-dados-do-meu-cv/ -- WorldCloud ##\n",
    "\n",
    "##5 https://www.youtube.com/watch?v=iQ1bfDMCv_c&list=PLf6b7z7NwdGVMXPEvoJu64jxtE58q4CH8&index=3&t=9s - Videos da ALICE ZHAO##\n",
    "#https://github.com/adashofdata/nlp-in-python-tutorial\n",
    "\n",
    "\n",
    "## 6 https://gist.github.com/alopes/5358189 - lista de StopWords em português com correspondências. \n",
    "\n",
    "\n",
    "## 7 https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e\n",
    "\n",
    "\n",
    "### 8 - https://www.youtube.com/watch?v=SF50IK5XgKA&t=600s - Willian Zeng - pyLDAvis com scikit-learn\n",
    "##https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb#topic=11&lambda=0.5&term=\n",
    "\n",
    "https://stackoverflow.com/questions/60748383/add-coustome-stopwords-list-in-countvectorizer -- Add stopwords com um arquivo\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/41094718/how-to-use-the-scikit-learn-countvectorizer\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3 and python 2\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules of Statistics and EDA to use if necessary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the pyLDAvis and some dependences of help\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the scikit-learn LDA \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words ## \n",
    "\n",
    "stop_words = get_stop_words('pt')\n",
    "stop_words = get_stop_words('portuguese')\n",
    "\n",
    "from stop_words import safe_get_stop_words\n",
    "\n",
    "stop_words = safe_get_stop_words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>registro da genealogia de jesus cristo filho d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a visita dos magos  depois que jesus nasceu em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naqueles dias surgiu joão batista pregando no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>então jesus foi levado pelo espírito ao deser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as bemaventuranças lc   vendo as multidões jes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts\n",
       "0  registro da genealogia de jesus cristo filho d...\n",
       "1  a visita dos magos  depois que jesus nasceu em...\n",
       "2   naqueles dias surgiu joão batista pregando no...\n",
       "3   então jesus foi levado pelo espírito ao deser...\n",
       "4  as bemaventuranças lc   vendo as multidões jes..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ler os dados aqui. \n",
    "data_df = pd.read_pickle(\"dados_limpos2.pkl\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['texts']\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "#Matriz \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data_df)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'era', 'esteja', 'das', 'estou', 'mas', 'como', 'pelos', 'houveram', 'aos', 'tivermos', 'é', 'estiver', 'tivéssemos', 'tenham', 'minha', 'você', 'as', 'tivesse', 'houveria', 'a', 'houverão', 'serei', 'meu', 'pelas', 'está', 'houver', 'forem', 'depois', 'tivera', 'essa', 'dos', 'isso', 'houvemos', 'seu', 'houverei', 'te', 'sejam', 'por', 'dela', 'tinham', 'à', 'estas', 'tivemos', 'isto', 'esses', 'haja', 'seríamos', 'houvéssemos', 'fosse', 'até', 'hajam', 'houveremos', 'fôssemos', 'teu', 'seus', 'também', 'aquilo', 'tiveram', 'houverá', 'houvéramos', 'tiverem', 'elas', 'nas', 'serão', 'teus', 'um', 'tua', 'houvessem', 'houveríamos', 'sou', 'fui', 'será', 'que', 'for', 'da', 'vos', 'nossa', 'tem', 'dele', 'tiver', 'estiverem', 'houvermos', 'nossos', 'ao', 'nossas', 'uma', 'este', 'estamos', 'foram', 'suas', 'o', 'numa', 'esta', 'aquelas', 'vocês', 'com', 'aquela', 'teremos', 'hajamos', 'estejam', 'hei', 'sem', 'fomos', 'aquele', 'tenho', 'temos', 'lhes', 'pela', 'qual', 'tinha', 'estávamos', 'estivesse', 'éramos', 'eram', 'me', 'terão', 'nosso', 'havemos', 'essas', 'tínhamos', 'estivéramos', 'são', 'fôramos', 'e', 'estivéssemos', 'não', 'nós', 'houverem', 'aqueles', 'estavam', 'os', 'nos', 'tém', 'eles', 'houvera', 'estivemos', 'mais', 'estes', 'de', 'terá', 'seria', 'formos', 'lhe', 'só', 'somos', 'às', 'pelo', 'quando', 'ou', 'entre', 'estivermos', 'esse', 'terei', 'em', 'seriam', 'já', 'ele', 'estão', 'fossem', 'ela', 'estive', 'teria', 'teve', 'tivéramos', 'na', 'teriam', 'eu', 'houve', 'nem', 'tuas', 'minhas', 'teríamos', 'estava', 'tenha', 'houvesse', 'muito', 'para', 'estejamos', 'tive', 'tivessem', 'hão', 'foi', 'esteve', 'sejamos', 'estivera', 'seremos', 'seja', 'estivessem', 'num', 'meus', 'houveriam', 'estiveram', 'do', 'há', 'deles', 'sua', 'quem', 'mesmo', 'tu', 'no', 'delas', 'fora', 'se', 'tenhamos'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Klemer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Klemer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WordListCorpusReader' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7588e3c87dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m                   \u001b[1;34m\"tenha\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tenhamos\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tenham\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivesse\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivéssemos\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivessem\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tiver\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivermos\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tiverem\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                   \"terei\",\"terá\",\"teremos\",\"terão\", \"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\"])\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewStopWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewStopWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mnewStopWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WordListCorpusReader' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "# Create a new document-term matrix using this new stopwords\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "\"\"\" \n",
    "stop_words = set(STOPWORDS)\n",
    "add_stop_words= [(\"de\",\"a\", \"o\", \"que\",\"e\",\"do\", \"da\", \"em\", \"um\", \"para\", \"é\", \"com\", \"não\", \"uma\", \"os\", \"no\", \"se\", \n",
    "                  \"na\", \"por\", \"mais\", \"as\", \"dos\", \"como\", \"mas\", \"foi\", \"ao\", \"ele\", \"das\", \"tem\", \"à\",\"às\",\n",
    "                  \"seu\", \"sua\",\"ou\",\"ser\", \"quando\", \"muito\", \"há\", \"nos\", \"já\", \"está\", \"eu\", \n",
    "                  \"também\", \"só\", \"pelo\", \"pela\", \"até\", \"isso\" ,\"ela\", \"entre\", \"era\", \n",
    "                  \"depois\", \"sem\", \"mesmo\", \"aos\", \"onde\",\"ter\", \"seus\", \"quem\", \"nas\", \"me\", \n",
    "                  \"esse\", \"eles\", \"estão\", \"você\",\"tinha\", \"foram\", \"essa\", \n",
    "                  \"num\", \"nem\", \"suas\", \"meu\", \"às\", \"minha\",\"têm\", \"numa\", \"pelos\", \"elas\", \n",
    "                  \"havia\", \"seja\", \"qual\", \"será\", \"nós\", \"tenho\", \"lhe\", \"deles\", \"essas\", \"esses\", \n",
    "                  \"pelas\", \"este\", \"fosse\", \"dele\", \"tu\", \"te\",\"vocês\",\"vos\", \"lhes\", \"meus\", \"minhas\", \n",
    "                  \"teu\", \"tua\", \"teus\", \"tuas\", \"nosso\", \"nossa\", \"nossos\",\"nossas\",\n",
    "                  \"dela\", \"delas\", \"esta\", \"estes\", \"estas\", \"aquele\", \"aquela\", \"aqueles\", \n",
    "                  \"aquelas\", \"isto\", \"aquilo\", \"estou\", \"está\",\"estamos\", \"estão\", \"estive\", \n",
    "                  \"esteve\", \"estivemos\", \"estiveram\",\"estava\", \"estávamos\", \"estavam\", \"estivera\", \n",
    "                  \"estivéramos\", \"esteja\",\"estejamos\", \"estejam\",\"estivesse\", \"estivéssemos\",\n",
    "                  \"estivessem\", \"estiver\", \"estivermos\",\"estiverem\", \"hein\", \"há\",\"havemos\", \"hão\", \"houve\", \n",
    "                  \"houvemos\", \"houveram\", \"houvera\",\"houvéramos\",\"haja\", \"hajamos\", \n",
    "                  \"hajam\",\"houvesse\", \"houvéssemos\", \"houvessem\", \"houver\",\"houvermos\", \n",
    "                  \"houverem\", \"houverei\", \"houverá\", \"houveremos\", \"houverão\", \"houveria\",\n",
    "                  \"houveríamos\", \"houveriam\" ,\"sou\" ,\"somos\",\"são\" ,\"era\", \"éramos\" ,\"eram\", \"fui\",\"foi\", \n",
    "                  \"fomos\", \"foram\", \"fora\", \"fôramos\", \"seja\",\"sejamos\", \"sejam\", \n",
    "                  \"fosse\", \"fôssemos\", \"fossem\", \"for\" ,\"formos\", \"forem\", \"serei\", \n",
    "                  \"será\", \"seremos\", \"serão\",\"seria\",\"seríamos\",\"seriam\",\"tenho\",\"tem\",\"temos\",\n",
    "                  \"têm\",\"tinha\" ,\"tínhamos\" ,\"tinham\" ,\"tive\",\"teve\" ,\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\n",
    "                  \"tenha\", \"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivessem\",\"tiver\",\"tivermos\",\"tiverem\",\n",
    "                  \"terei\",\"terá\",\"teremos\",\"terão\", \"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\")]\n",
    "data_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# python-3 https://stackoverflow.com/questions/16681712/python-doesnt-interpret-utf8-correctly\n",
    "with open('utf8.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines() # returns unicode \n",
    "\n",
    "#with open('./stopwords.txt', mode='r',encoding='utf-8') as stop_words:\n",
    "    list_stop_words = stop_words.read().split()\n",
    "    \n",
    "print(list_stop_words) \n",
    "\"\"\"\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "nltk.download ('stopwords')\n",
    "nltk.download ('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('portuguese'))\n",
    "print(stop_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordListCorpusReader' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0d3eda5e61b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m                   \u001b[1;34m\"tenha\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tenhamos\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tenham\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivesse\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivéssemos\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivessem\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tiver\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tivermos\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"tiverem\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                   \"terei\",\"terá\",\"teremos\",\"terão\", \"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\"])\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewStopWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewStopWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mnewStopWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WordListCorpusReader' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('portuguese'))\n",
    "newStopWords = ([\"de\",\"a\", \"o\", \"que\",\"e\",\"do\", \"da\", \"em\", \"um\", \"para\", \"é\", \"com\", \"não\", \"uma\", \"os\", \n",
    "                  \"no\", \"se\",\"na\", \"por\",\"mais\", \"as\", \"dos\", \"como\", \"mas\", \"foi\", \"ao\", \"ele\", \"das\", \"tem\",\n",
    "                  \"à\",\"às\",\"seu\", \"sua\",\"ou\",\"ser\", \"quando\", \"muito\", \"há\", \"nos\", \"já\", \"está\", \"eu\", \n",
    "                  \"também\", \"só\",\"pelo\", \"pela\",\"até\", \"isso\" ,\"ela\", \"entre\", \"era\", \"depois\", \"sem\", \"mesmo\", \n",
    "                  \"aos\", \"onde\",\"ter\", \"seus\", \"quem\", \"nas\", \"me\", \"esse\", \"eles\", \"estão\", \n",
    "                  \"você\",\"tinha\", \"foram\", \"essa\",\"num\", \"nem\", \"suas\", \"meu\", \"às\", \"minha\",\"têm\",\n",
    "                  \"numa\", \"pelos\", \"elas\",\"havia\", \"seja\", \"qual\", \"será\", \"nós\", \"tenho\", \n",
    "                  \"lhe\", \"deles\", \"essas\", \"esses\",\"pelas\", \"este\", \"fosse\", \"dele\",\"tu\", \"te\",\n",
    "                  \"vocês\",\"vos\", \"lhes\", \"meus\", \"minhas\", \"teu\",\"tua\", \"teus\", \"tuas\", \"nosso\", \n",
    "                  \"nossa\", \"nossos\",\"nossas\", \"dela\", \"delas\", \"esta\", \"estes\",\n",
    "                  \"estas\", \"aquele\", \"aquela\", \"aqueles\", \"aquelas\", \"isto\", \"aquilo\", \"estou\", \"está\", \n",
    "                  \"estamos\", \"estão\", \"estive\", \"esteve\", \"estivemos\", \"estiveram\", \"estava\", \"estávamos\", \n",
    "                  \"estavam\", \"estivera\", \"estivéramos\", \"esteja\",\"estejamos\", \"estejam\",\"estivesse\", \"estivéssemos\",\n",
    "                  \"estivessem\", \"estiver\", \"estivermos\",\"estiverem\", \"hein\", \"há\", \"havemos\", \"hão\", \"houve\", \n",
    "                  \"houvemos\", \"houveram\", \"houvera\",\"houvéramos\", \"haja\", \"hajamos\", \"hajam\",\"houvesse\", \n",
    "                  \"houvéssemos\",\"houvessem\", \"houver\", \"houvermos\", \"houverem\", \"houverei\", \n",
    "                  \"houverá\", \"houveremos\", \"houverão\", \"houveria\",\"houveríamos\", \"houveriam\" ,\"sou\" ,\"somos\",\n",
    "                  \"são\" ,\"era\", \"éramos\" ,\"eram\", \"fui\", \"foi\", \"fomos\", \"foram\", \"fora\", \"fôramos\", \"seja\", \n",
    "                  \"sejamos\", \"sejam\", \"fosse\", \"fôssemos\", \"fossem\", \"for\" ,\"formos\", \"forem\", \"serei\", \n",
    "                  \"será\", \"seremos\", \"serão\",\"seria\",\"seríamos\" ,\"seriam\" ,\"tenho\",\"tem\",\"temos\",\n",
    "                  \"têm\",\"tinha\" ,\"tínhamos\" ,\"tinham\" ,\"tive\" ,\"teve\" ,\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\n",
    "                  \"tenha\", \"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivessem\",\"tiver\",\"tivermos\",\"tiverem\",\n",
    "                  \"terei\",\"terá\",\"teremos\",\"terão\", \"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\"])\n",
    "stopwords.extend(newStopWords) \n",
    "for i in newStopWords:\n",
    "    newStopWords.append(i)\n",
    "print(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not a built-in stop list: newStopWords",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-889bfdcb333c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                 \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                 min_df = 10)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdtm_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1199\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m         \u001b[0manalyze\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[0mj_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mbuild_analyzer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'word'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m             \u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             self._check_stop_words_consistency(stop_words, preprocess,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mget_stop_words\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstop\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \"\"\"\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_check_stop_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_stop_words_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_stop_list\u001b[1;34m(stop)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"not a built-in stop list: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not a built-in stop list: newStopWords"
     ]
    }
   ],
   "source": [
    "# Recreate a document-term matrix \n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                encoding='utf-8',\n",
    "                                stop_words = 'newStopWords', #troque pelo nome do arquivo , no lugar de 'english'\n",
    "                                lowercase= True,\n",
    "                                token_pattern = r'\\b[a-zA-Z0-9]{3,}\\b',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(data_df)\n",
    "print(dtm_tf)\n",
    "print(dtm_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4c0cb5d4211a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdtm_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(data_df)\n",
    "print(dtm_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e700dd7d577b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# for TF DTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlda_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mlda_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# for TFIDF DTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_topics'"
     ]
    }
   ],
   "source": [
    "#Create a LDA model \n",
    "#lda_tf = LatentDirichletAllocation(n_topics=30,n_components=20,random_state=0)\n",
    "#lda_tf.fit(dtm_tf)\n",
    "\n",
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_topics=20, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_topics=20, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization Finally, the LDA models are fitted.\n",
    "#pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
