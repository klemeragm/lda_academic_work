{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "## 1 https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0 (Testa esse aqui tbm)\n",
    "##Auctor: Shashank Kapadia\n",
    "\n",
    "## 2 https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html \n",
    "\n",
    "##3 https://sigmoidal.ai/como-criar-uma-wordcloud-em-python/  -- WorldClouds ## \n",
    "##4 http://tirandolicoesdetudo.com.br/criando-uma-nuvem-de-palavras-wordcloud-com-dados-do-meu-cv/ -- WorldCloud ##\n",
    "\n",
    "##5 https://www.youtube.com/watch?v=iQ1bfDMCv_c&list=PLf6b7z7NwdGVMXPEvoJu64jxtE58q4CH8&index=3&t=9s - Videos da ALICE ZHAO##\n",
    "#https://github.com/adashofdata/nlp-in-python-tutorial\n",
    "\n",
    "\n",
    "## 6 https://gist.github.com/alopes/5358189 - lista de StopWords em português com correspondências. \n",
    "\n",
    "\n",
    "## 7 https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e\n",
    "\n",
    "\n",
    "### 8 - https://www.youtube.com/watch?v=SF50IK5XgKA&t=600s - Willian Zeng - pyLDAvis com scikit-learn\n",
    "##https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb#topic=11&lambda=0.5&term=\n",
    "\n",
    "https://stackoverflow.com/questions/60748383/add-coustome-stopwords-list-in-countvectorizer -- Add stopwords com um arquivo\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/41094718/how-to-use-the-scikit-learn-countvectorizer\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3 and python 2\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules of Statistics and EDA to use if necessary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the pyLDAvis and some dependences of help\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the scikit-learn LDA \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words ## \n",
    "\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "from stop_words import safe_get_stop_words\n",
    "\n",
    "stop_words = safe_get_stop_words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a genealogia de jesus   registro da genealogia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a visita dos magos   depois que jesus nasceu e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joão batista prepara o caminho  mc       lc   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a tentação de jesus  mc     lc          então ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as bem aventuranças  lc            vendo as mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc\n",
       "0  a genealogia de jesus   registro da genealogia...\n",
       "1  a visita dos magos   depois que jesus nasceu e...\n",
       "2  joão batista prepara o caminho  mc       lc   ...\n",
       "3  a tentação de jesus  mc     lc          então ...\n",
       "4  as bem aventuranças  lc            vendo as mu..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ler os dados aqui. \n",
    "data_df = pd.read_pickle(\"dados_limpos.pkl\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc']\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "#Matriz \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data_df)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'é', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi', 'ao', 'ele', 'das', 'tem', 'à', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito', 'há', 'nos', 'já', 'está', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'era', 'depois', 'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'estão', 'você', 'tinha', 'foram', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'têm', 'numa', 'pelos', 'elas', 'havia', 'seja', 'qual', 'será', 'nós', 'tenho', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'fosse', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "# Create a new document-term matrix using this new stopwords\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "\"\"\" \n",
    "stop_words = set(STOPWORDS)\n",
    "add_stop_words= [(\"de\",\"a\", \"o\", \"que\",\"e\",\"do\", \"da\", \"em\", \"um\", \"para\", \"é\", \"com\", \"não\", \"uma\", \"os\", \"no\", \"se\", \n",
    "                  \"na\", \"por\", \"mais\", \"as\", \"dos\", \"como\", \"mas\", \"foi\", \"ao\", \"ele\", \"das\", \"tem\", \"à\",\"às\",\n",
    "                  \"seu\", \"sua\",\"ou\",\"ser\", \"quando\", \"muito\", \"há\", \"nos\", \"já\", \"está\", \"eu\", \n",
    "                  \"também\", \"só\", \"pelo\", \"pela\", \"até\", \"isso\" ,\"ela\", \"entre\", \"era\", \n",
    "                  \"depois\", \"sem\", \"mesmo\", \"aos\", \"onde\",\"ter\", \"seus\", \"quem\", \"nas\", \"me\", \n",
    "                  \"esse\", \"eles\", \"estão\", \"você\",\"tinha\", \"foram\", \"essa\", \n",
    "                  \"num\", \"nem\", \"suas\", \"meu\", \"às\", \"minha\",\"têm\", \"numa\", \"pelos\", \"elas\", \n",
    "                  \"havia\", \"seja\", \"qual\", \"será\", \"nós\", \"tenho\", \"lhe\", \"deles\", \"essas\", \"esses\", \n",
    "                  \"pelas\", \"este\", \"fosse\", \"dele\", \"tu\", \"te\",\"vocês\",\"vos\", \"lhes\", \"meus\", \"minhas\", \n",
    "                  \"teu\", \"tua\", \"teus\", \"tuas\", \"nosso\", \"nossa\", \"nossos\",\"nossas\",\n",
    "                  \"dela\", \"delas\", \"esta\", \"estes\", \"estas\", \"aquele\", \"aquela\", \"aqueles\", \n",
    "                  \"aquelas\", \"isto\", \"aquilo\", \"estou\", \"está\",\"estamos\", \"estão\", \"estive\", \n",
    "                  \"esteve\", \"estivemos\", \"estiveram\",\"estava\", \"estávamos\", \"estavam\", \"estivera\", \n",
    "                  \"estivéramos\", \"esteja\",\"estejamos\", \"estejam\",\"estivesse\", \"estivéssemos\",\n",
    "                  \"estivessem\", \"estiver\", \"estivermos\",\"estiverem\", \"hein\", \"há\",\"havemos\", \"hão\", \"houve\", \n",
    "                  \"houvemos\", \"houveram\", \"houvera\",\"houvéramos\",\"haja\", \"hajamos\", \n",
    "                  \"hajam\",\"houvesse\", \"houvéssemos\", \"houvessem\", \"houver\",\"houvermos\", \n",
    "                  \"houverem\", \"houverei\", \"houverá\", \"houveremos\", \"houverão\", \"houveria\",\n",
    "                  \"houveríamos\", \"houveriam\" ,\"sou\" ,\"somos\",\"são\" ,\"era\", \"éramos\" ,\"eram\", \"fui\",\"foi\", \n",
    "                  \"fomos\", \"foram\", \"fora\", \"fôramos\", \"seja\",\"sejamos\", \"sejam\", \n",
    "                  \"fosse\", \"fôssemos\", \"fossem\", \"for\" ,\"formos\", \"forem\", \"serei\", \n",
    "                  \"será\", \"seremos\", \"serão\",\"seria\",\"seríamos\",\"seriam\",\"tenho\",\"tem\",\"temos\",\n",
    "                  \"têm\",\"tinha\" ,\"tínhamos\" ,\"tinham\" ,\"tive\",\"teve\" ,\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\n",
    "                  \"tenha\", \"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivessem\",\"tiver\",\"tivermos\",\"tiverem\",\n",
    "                  \"terei\",\"terá\",\"teremos\",\"terão\", \"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\")]\n",
    "data_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# python-3 https://stackoverflow.com/questions/16681712/python-doesnt-interpret-utf8-correctly\n",
    "with open('utf8.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines() # returns unicode \n",
    "\"\"\"\n",
    "\n",
    "with open('./stopwords.txt', mode='r',encoding='utf-8') as stop_words:\n",
    "    list_stop_words = stop_words.read().split()\n",
    "    \n",
    "\n",
    "\n",
    "print(list_stop_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-235dade8fe8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Recreate a document-term matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m tf_vectorizer = CountVectorizer(strip_accents='unicode',\n\u001b[0m\u001b[0;32m      3\u001b[0m                                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                 \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'list_stop_words'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#troque pelo nome do arquivo , no lugar de 'english'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Recreate a document-term matrix \n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                encoding='utf-8',\n",
    "                                stop_words = 'list_stop_words', #troque pelo nome do arquivo , no lugar de 'english'\n",
    "                                lowercase= True,\n",
    "                                token_pattern = r'\\b[a-zA-Z0-9]{3,}\\b',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(data_df)\n",
    "print(dtm_tf)\n",
    "print(dtm_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4c0cb5d4211a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdtm_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(data_df)\n",
    "print(dtm_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e700dd7d577b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# for TF DTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlda_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mlda_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtm_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# for TFIDF DTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_topics'"
     ]
    }
   ],
   "source": [
    "#Create a LDA model \n",
    "#lda_tf = LatentDirichletAllocation(n_topics=30,n_components=20,random_state=0)\n",
    "#lda_tf.fit(dtm_tf)\n",
    "\n",
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_topics=20, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_topics=20, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization Finally, the LDA models are fitted.\n",
    "#pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
