{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0 \n",
    "##Auctor: Shashank Kapadia -- 2019  (Testa esse aqui tbm)\n",
    "\n",
    "## 2 https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html -- doc\n",
    "\n",
    "##3 https://sigmoidal.ai/como-criar-uma-wordcloud-em-python/  -- WorldClouds ## \n",
    "##4 http://tirandolicoesdetudo.com.br/criando-uma-nuvem-de-palavras-wordcloud-com-dados-do-meu-cv/ -- WorldCloud ##\n",
    "\n",
    "##5 https://www.youtube.com/watch?v=iQ1bfDMCv_c&list=PLf6b7z7NwdGVMXPEvoJu64jxtE58q4CH8&index=3&t=9s - Videos da ALICE ZHAO##\n",
    "#https://github.com/adashofdata/nlp-in-python-tutorial - 2018\n",
    "\n",
    "\n",
    "## 6 https://gist.github.com/alopes/5358189 - lista de StopWords em português com correspondências. \n",
    "\n",
    "\n",
    "## 7 https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e\n",
    "\n",
    "##8 - https://medium.com/somos-tera/como-modelar-t%C3%B3picos-atrav%C3%A9s-de-latent-dirichlet-allocation-lda-atrav%C3%A9s-da-biblioteca-gensim-1fa17357ad4b -- 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3 and python 2 #para puxar as bibliotecas que estejam disponíveis na versões. \n",
    "from __future__ import print_function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "import sys\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caps</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mateus 1</td>\n",
       "      <td>Registro da genealogia de Jesus Cristo filho d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Visita dos Magos 1 Depois que Jesus nasceu e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1 Naqueles dias surgiu João Batista pregando n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1 Então Jesus foi levado pelo Espírito ao dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>As Bem-aventuranças (Lc 6.20-23) 1 Vendo as mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>A Ajuda aos Necessitados 1 “Tenham o cuidado d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1 “Não julguem para que vocês não sejam julgad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1 Quando ele desceu do monte grandes multidões...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1 Entrando Jesus num barco atravessou o mar e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1 Chamando seus doze discípulos deu-lhes autor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       caps                                              texts\n",
       "0  Mateus 1  Registro da genealogia de Jesus Cristo filho d...\n",
       "1         2  A Visita dos Magos 1 Depois que Jesus nasceu e...\n",
       "2         3  1 Naqueles dias surgiu João Batista pregando n...\n",
       "3         4  1 Então Jesus foi levado pelo Espírito ao dese...\n",
       "4         5  As Bem-aventuranças (Lc 6.20-23) 1 Vendo as mu...\n",
       "5         6  A Ajuda aos Necessitados 1 “Tenham o cuidado d...\n",
       "6         7  1 “Não julguem para que vocês não sejam julgad...\n",
       "7         8  1 Quando ele desceu do monte grandes multidões...\n",
       "8         9  1 Entrando Jesus num barco atravessou o mar e ...\n",
       "9        10  1 Chamando seus doze discípulos deu-lhes autor..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ler os dados aqui. \n",
    "data_df = pd.read_csv('./datasets/docs_gospels_todos.csv', sep=';', encoding='utf-8')\n",
    "#Imprima as linhas com .head()\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef combined_text(docs):\\n    combined_text = ' '.join(docs)\\n    return combined_text\\n\\n##Dados combinados dentro da coluna\\ndocs = {key: [combine_text(value)] for (key,value) in docs.items}\\n\\npd.set_option('max_colwidth',150)\\n\\ndocs= pd.DataFrame.from_dict(data_combined).transpose()\\ndocs.columns = ['doc']\\ndocs = docs.sort_index()\\ndocs \\n\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def combined_text(docs):\n",
    "    combined_text = ' '.join(docs)\n",
    "    return combined_text\n",
    "\n",
    "##Dados combinados dentro da coluna\n",
    "docs = {key: [combine_text(value)] for (key,value) in docs.items}\n",
    "\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "docs= pd.DataFrame.from_dict(data_combined).transpose()\n",
    "docs.columns = ['doc']\n",
    "docs = docs.sort_index()\n",
    "docs \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Registro da genealogia de Jesus Cristo filho d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Visita dos Magos 1 Depois que Jesus nasceu e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Naqueles dias surgiu João Batista pregando n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Então Jesus foi levado pelo Espírito ao dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As Bem-aventuranças (Lc 6.20-23) 1 Vendo as mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts\n",
       "0  Registro da genealogia de Jesus Cristo filho d...\n",
       "1  A Visita dos Magos 1 Depois que Jesus nasceu e...\n",
       "2  1 Naqueles dias surgiu João Batista pregando n...\n",
       "3  1 Então Jesus foi levado pelo Espírito ao dese...\n",
       "4  As Bem-aventuranças (Lc 6.20-23) 1 Vendo as mu..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removendo a coluna de capítulos, pois não precisamos das mesmas. \n",
    "data_df = data_df.drop('caps', axis=1) #df.drop('column_name', axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "#next(iter(data_df.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "#next(iter(data_df.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ncombined_text=0\\ndef combine_text(doc):\\n   '''Takes a list of text and combines them into one large chunk of text.'''\\n    combined_text = ' '.join(doc)\\n    return combined_text\\n\\nprint(combined_text)\\n\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "combined_text=0\n",
    "def combine_text(doc):\n",
    "   '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(doc)\n",
    "    return combined_text\n",
    "\n",
    "print(combined_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation tutorial 5\n",
    "import re\n",
    "import string \n",
    "\n",
    "def clean_text_docs_round1(texts):\n",
    "        texts= texts.lower()\n",
    "        texts= re.sub('[.*-,/?!:;()\\']', ' ',texts)\n",
    "        texts= re.sub('[-|0:9]', ' ', texts)\n",
    "        texts= re.sub('[%s]' % re.escape(string.punctuation), ' ',texts)\n",
    "        texts= re.sub('\\w*\\d\\w*', ' ',texts)\n",
    "        return texts\n",
    "    \n",
    "round1 = lambda x: clean_text_docs_round1(x)\n",
    "    \n",
    "# Convert the titles to lowercase\n",
    "#docs['doc'] = docs['doc'].map(lambda x: x.lower())\n",
    "#docs['doc'] = docs['doc'].map(lambda x: re.sub('[,\\.!?^~´`0:9]', ' ', x))\n",
    "# Print out the first rows of papers\n",
    "#docs['doc'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>registro da genealogia de jesus cristo filho d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a visita dos magos   depois que jesus nasceu e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naqueles dias surgiu joão batista pregando n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>então jesus foi levado pelo espírito ao dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as bem aventuranças  lc           vendo as mul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts\n",
       "0  registro da genealogia de jesus cristo filho d...\n",
       "1  a visita dos magos   depois que jesus nasceu e...\n",
       "2    naqueles dias surgiu joão batista pregando n...\n",
       "3    então jesus foi levado pelo espírito ao dese...\n",
       "4  as bem aventuranças  lc           vendo as mul..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Atualizar documento com o texto atualizado fase 1\n",
    "data_clean = pd.DataFrame(data_df.texts.apply(round1))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(texts):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    texts = re.sub('[.*‘’“”]', ' ', texts)\n",
    "    texts = re.sub('\\r', ' ', texts)\n",
    "    texts = re.sub('\\n', ' ', texts)\n",
    "    return texts\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>registro da genealogia de jesus cristo filho d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a visita dos magos   depois que jesus nasceu e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naqueles dias surgiu joão batista pregando n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>então jesus foi levado pelo espírito ao dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as bem aventuranças  lc           vendo as mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a ajuda aos necessitados    tenham o cuidado d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>não julguem para que vocês não sejam julgad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quando ele desceu do monte grandes multidões...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>entrando jesus num barco atravessou o mar e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chamando seus doze discípulos deu lhes autor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts\n",
       "0  registro da genealogia de jesus cristo filho d...\n",
       "1  a visita dos magos   depois que jesus nasceu e...\n",
       "2    naqueles dias surgiu joão batista pregando n...\n",
       "3    então jesus foi levado pelo espírito ao dese...\n",
       "4  as bem aventuranças  lc           vendo as mul...\n",
       "5  a ajuda aos necessitados    tenham o cuidado d...\n",
       "6     não julguem para que vocês não sejam julgad...\n",
       "7    quando ele desceu do monte grandes multidões...\n",
       "8    entrando jesus num barco atravessou o mar e ...\n",
       "9    chamando seus doze discípulos deu lhes autor..."
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean.texts.apply(round2))\n",
    "data_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amazenar os dados em um formato pickle para ulilizado posteriormente\n",
    "#import pickle\n",
    "data_clean.to_pickle('dados_limpos.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce891266e2a4a6894f5d36923053af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_clean['lang'] = data_clean.texts.progress_map(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pt    91\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.loc[data_clean.lang=='pt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Klemer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf80f80501bd407c93b3e44f5ed753cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_clean['sentences'] = data_clean.texts.progress_map(sent_tokenize)\n",
    "    \n",
    "#data_clean['sentences'].head(1).tolist()[0][:3] # Print the first 3 sentences of the 1st article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287be2b824734d61998ab2b6b629401c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[['registro', 'da', 'genealogia', 'de', 'jesus', 'cristo', 'filho', 'de', 'davi', 'filho', 'de', 'abraão', 'abraão', 'gerou', 'isaque', 'isaque', 'gerou', 'jacó', 'jacó', 'gerou', 'judá', 'e', 'seus', 'irmãos', 'judá', 'gerou', 'perez', 'e', 'zerá', 'cuja', 'mãe', 'foi', 'tamar', 'perez', 'gerou', 'esrom', 'esrom', 'gerou', 'arão', 'arão', 'gerou', 'aminadabe', 'aminadabe', 'gerou', 'naassom', 'naassom', 'gerou', 'salmom', 'salmom', 'gerou', 'boaz', 'cuja', 'mãe', 'foi', 'raabe', 'boaz', 'gerou', 'obede', 'cuja', 'mãe', 'foi', 'rute', 'obede', 'gerou', 'jessé', 'e', 'jessé', 'gerou', 'o', 'rei', 'davi', 'davi', 'gerou', 'salomão', 'cuja', 'mãe', 'tinha', 'sido', 'mulher', 'de', 'urias', 'salomão', 'gerou', 'roboão', 'roboão', 'gerou', 'abias', 'abias', 'gerou', 'asa', 'asa', 'gerou', 'josafá', 'josafá', 'gerou', 'jorão', 'jorão', 'gerou', 'uzias', 'uzias', 'gerou', 'jotão', 'jotão', 'gerou', 'acaz', 'acaz', 'gerou', 'ezequias', 'ezequias', 'gerou', 'manassés', 'manassés', 'gerou', 'amom', 'amom', 'gerou', 'josias', 'e', 'josias', 'gerou', 'jeconiasa', 'e', 'seus', 'irmãos', 'no', 'tempo', 'do', 'exílio', 'na', 'babilônia', 'depois', 'do', 'exílio', 'na', 'babilônia', 'jeconias', 'gerou', 'salatiel', 'salatiel', 'gerou', 'zorobabel', 'zorobabel', 'gerou', 'abiúde', 'abiúde', 'gerou', 'eliaquim', 'eliaquim', 'gerou', 'azor', 'azor', 'gerou', 'sadoque', 'sadoque', 'gerou', 'aquim', 'aquim', 'gerou', 'eliúde', 'eliúde', 'gerou', 'eleazar', 'eleazar', 'gerou', 'matã', 'matã', 'gerou', 'jacó', 'e', 'jacó', 'gerou', 'josé', 'marido', 'de', 'maria', 'da', 'qual', 'nasceu', 'jesus', 'que', 'é', 'chamado', 'cristo', 'assim', 'ao', 'todo', 'houve', 'catorze', 'gerações', 'de', 'abraão', 'a', 'davi', 'catorze', 'de', 'davi', 'até', 'o', 'exílio', 'na', 'babilônia', 'e', 'catorze', 'do', 'exílio', 'até', 'o', 'cristo', 'o', 'nascimento', 'de', 'jesus', 'cristo', 'lc', 'foi', 'assim', 'o', 'nascimento', 'de', 'jesus', 'cristo', 'maria', 'sua', 'mãe', 'estava', 'prometida', 'em', 'casamento', 'a', 'josé', 'mas', 'antes', 'que', 'se', 'unissem', 'achou', 'se', 'grávida', 'pelo', 'espírito', 'santo', 'por', 'ser', 'josé', 'seu', 'marido', 'um', 'homem', 'justo', 'e', 'não', 'querendo', 'expô', 'la', 'à', 'desonra', 'pública', 'pretendia', 'anular', 'o', 'casamento', 'secretamente', 'mas', 'depois', 'de', 'ter', 'pensado', 'nisso', 'apareceu', 'lhe', 'um', 'anjo', 'do', 'senhor', 'em', 'sonho', 'e', 'disse', 'josé', 'filho', 'de', 'davi', 'não', 'tema', 'receber', 'maria', 'como', 'sua', 'esposa', 'pois', 'o', 'que', 'nela', 'foi', 'gerado', 'procede', 'do', 'espírito', 'santo', 'ela', 'dará', 'à', 'luz', 'um', 'filho', 'e', 'você', 'deverá', 'dar', 'lhe', 'o', 'nome', 'de', 'jesusb', 'porque', 'ele', 'salvará', 'o', 'seu', 'povo', 'dos', 'seus', 'pecados', 'tudo', 'isso', 'aconteceu', 'para', 'que', 'se', 'cumprisse', 'o', 'que', 'o', 'senhor', 'dissera', 'pelo', 'profeta', 'a', 'virgem', 'ficará', 'grávida', 'dará', 'à', 'luz', 'um', 'filho', 'e', 'lhe', 'chamarão', 'emanuel', 'que', 'significa', 'deus', 'conosco', 'ao', 'acordar', 'josé', 'fez', 'o', 'que', 'o', 'anjo', 'do', 'senhor', 'lhe', 'tinha', 'ordenado', 'e', 'recebeu', 'maria', 'como', 'sua', 'esposa', 'mas', 'não', 'teve', 'relações', 'com', 'ela', 'enquanto', 'ela', 'não', 'deu', 'à', 'luz', 'um', 'filho', 'e', 'ele', 'lhe', 'pôs', 'o', 'nome', 'de', 'jesus']]\n"
     ]
    }
   ],
   "source": [
    "data_clean['tokens_sentences'] = data_clean['sentences'].progress_map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "#print(data_clean['tokens_sentences'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983eb413ef18408ea8ffcace387183b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[('registro', 'NN'), ('da', 'NN'), ('genealogia', 'NN'), ('de', 'IN'), ('jesus', 'NN'), ('cristo', 'NN'), ('filho', 'NN'), ('de', 'IN'), ('davi', 'FW'), ('filho', 'FW'), ('de', 'FW'), ('abraão', 'FW'), ('abraão', 'FW'), ('gerou', 'FW'), ('isaque', 'NN'), ('isaque', 'NN'), ('gerou', 'NN'), ('jacó', 'NN'), ('jacó', 'NN'), ('gerou', 'NN'), ('judá', 'NN'), ('e', 'NN'), ('seus', 'NN'), ('irmãos', 'NN'), ('judá', 'NN'), ('gerou', 'NN'), ('perez', 'NN'), ('e', 'NN'), ('zerá', 'NN'), ('cuja', 'NN'), ('mãe', 'NN'), ('foi', 'NN'), ('tamar', 'NN'), ('perez', 'NN'), ('gerou', 'NN'), ('esrom', 'IN'), ('esrom', 'JJ'), ('gerou', 'NN'), ('arão', 'NN'), ('arão', 'NN'), ('gerou', 'NN'), ('aminadabe', 'NN'), ('aminadabe', 'NN'), ('gerou', 'NN'), ('naassom', 'JJ'), ('naassom', 'JJ'), ('gerou', 'NN'), ('salmom', 'NN'), ('salmom', 'NN'), ('gerou', 'NN'), ('boaz', 'NN'), ('cuja', 'NN'), ('mãe', 'NN'), ('foi', 'NN'), ('raabe', 'NN'), ('boaz', 'NN'), ('gerou', 'NN'), ('obede', 'VBP'), ('cuja', 'NN'), ('mãe', 'NN'), ('foi', 'VBP'), ('rute', 'NN'), ('obede', 'NN'), ('gerou', 'NN'), ('jessé', 'NN'), ('e', 'NN'), ('jessé', 'NN'), ('gerou', 'NN'), ('o', 'NN'), ('rei', 'NN'), ('davi', 'NN'), ('davi', 'NN'), ('gerou', 'NN'), ('salomão', 'NN'), ('cuja', 'NN'), ('mãe', 'NN'), ('tinha', 'NN'), ('sido', 'NN'), ('mulher', 'FW'), ('de', 'FW'), ('urias', 'FW'), ('salomão', 'FW'), ('gerou', 'FW'), ('roboão', 'FW'), ('roboão', 'FW'), ('gerou', 'NN'), ('abias', 'NN'), ('abias', 'VBP'), ('gerou', 'NN'), ('asa', 'NN'), ('asa', 'NN'), ('gerou', 'NN'), ('josafá', 'NN'), ('josafá', 'NN'), ('gerou', 'NN'), ('jorão', 'NN'), ('jorão', 'NN'), ('gerou', 'NN'), ('uzias', 'JJ'), ('uzias', 'JJ'), ('gerou', 'NN'), ('jotão', 'NN'), ('jotão', 'NN'), ('gerou', 'NN'), ('acaz', 'NN'), ('acaz', 'NN'), ('gerou', 'NN'), ('ezequias', 'NN'), ('ezequias', 'FW'), ('gerou', 'FW'), ('manassés', 'FW'), ('manassés', 'FW'), ('gerou', 'FW'), ('amom', 'FW'), ('amom', 'FW'), ('gerou', 'NN'), ('josias', 'NN'), ('e', 'NN'), ('josias', 'NN'), ('gerou', 'NN'), ('jeconiasa', 'NN'), ('e', 'NN'), ('seus', 'VBD'), ('irmãos', 'JJ'), ('no', 'DT'), ('tempo', 'NN'), ('do', 'VBP'), ('exílio', 'VB'), ('na', 'TO'), ('babilônia', 'VB'), ('depois', 'NN'), ('do', 'VBP'), ('exílio', 'VB'), ('na', 'TO'), ('babilônia', 'VB'), ('jeconias', 'NN'), ('gerou', 'NN'), ('salatiel', 'NN'), ('salatiel', 'NN'), ('gerou', 'NN'), ('zorobabel', 'NN'), ('zorobabel', 'NN'), ('gerou', 'NN'), ('abiúde', 'NN'), ('abiúde', 'NN'), ('gerou', 'NN'), ('eliaquim', 'NN'), ('eliaquim', 'NN'), ('gerou', 'NN'), ('azor', 'NN'), ('azor', 'NN'), ('gerou', 'NN'), ('sadoque', 'NN'), ('sadoque', 'NN'), ('gerou', 'NN'), ('aquim', 'NN'), ('aquim', 'NN'), ('gerou', 'NN'), ('eliúde', 'NN'), ('eliúde', 'NN'), ('gerou', 'NN'), ('eleazar', 'NN'), ('eleazar', 'NN'), ('gerou', 'NN'), ('matã', 'NN'), ('matã', 'NN'), ('gerou', 'NN'), ('jacó', 'NN'), ('e', 'NN'), ('jacó', 'NN'), ('gerou', 'NN'), ('josé', 'NN'), ('marido', 'NN'), ('de', 'IN'), ('maria', 'FW'), ('da', 'FW'), ('qual', 'JJ'), ('nasceu', 'NN'), ('jesus', 'NN'), ('que', 'NN'), ('é', 'NNP'), ('chamado', 'NN'), ('cristo', 'NN'), ('assim', 'NN'), ('ao', 'NN'), ('todo', 'NN'), ('houve', 'NN'), ('catorze', 'NN'), ('gerações', 'NN'), ('de', 'IN'), ('abraão', 'FW'), ('a', 'DT'), ('davi', 'NN'), ('catorze', 'NN'), ('de', 'IN'), ('davi', 'FW'), ('até', 'JJ'), ('o', 'NN'), ('exílio', 'NN'), ('na', 'TO'), ('babilônia', 'VB'), ('e', 'NN'), ('catorze', 'NNS'), ('do', 'VBP'), ('exílio', 'RB'), ('até', 'VB'), ('o', 'JJ'), ('cristo', 'NN'), ('o', 'NN'), ('nascimento', 'FW'), ('de', 'FW'), ('jesus', 'FW'), ('cristo', 'NN'), ('lc', 'NN'), ('foi', 'NN'), ('assim', 'NN'), ('o', 'FW'), ('nascimento', 'FW'), ('de', 'FW'), ('jesus', 'FW'), ('cristo', 'NN'), ('maria', 'NNS'), ('sua', 'VBP'), ('mãe', 'JJ'), ('estava', 'NN'), ('prometida', 'NN'), ('em', 'NN'), ('casamento', 'VBZ'), ('a', 'DT'), ('josé', 'NN'), ('mas', 'NN'), ('antes', 'VBZ'), ('que', 'JJ'), ('se', 'JJ'), ('unissem', 'JJ'), ('achou', 'NN'), ('se', 'NN'), ('grávida', 'NN'), ('pelo', 'NN'), ('espírito', 'NN'), ('santo', 'NN'), ('por', 'NN'), ('ser', 'NN'), ('josé', 'NN'), ('seu', 'NN'), ('marido', 'NN'), ('um', 'JJ'), ('homem', 'NN'), ('justo', 'NN'), ('e', 'NN'), ('não', 'FW'), ('querendo', 'NN'), ('expô', 'NN'), ('la', 'NN'), ('à', 'NNP'), ('desonra', 'NN'), ('pública', 'NN'), ('pretendia', 'IN'), ('anular', 'JJ'), ('o', 'NN'), ('casamento', 'NN'), ('secretamente', 'NN'), ('mas', 'FW'), ('depois', 'NN'), ('de', 'IN'), ('ter', 'NN'), ('pensado', 'NN'), ('nisso', 'JJ'), ('apareceu', 'NN'), ('lhe', 'NN'), ('um', 'JJ'), ('anjo', 'NN'), ('do', 'VBP'), ('senhor', 'VB'), ('em', 'VB'), ('sonho', 'JJ'), ('e', 'NN'), ('disse', 'NN'), ('josé', 'NN'), ('filho', 'NN'), ('de', 'IN'), ('davi', 'FW'), ('não', 'JJ'), ('tema', 'NN'), ('receber', 'NN'), ('maria', 'NN'), ('como', 'NN'), ('sua', 'NN'), ('esposa', 'NN'), ('pois', 'NN'), ('o', 'NN'), ('que', 'NN'), ('nela', 'JJ'), ('foi', 'NN'), ('gerado', 'NN'), ('procede', 'NN'), ('do', 'VBP'), ('espírito', 'VB'), ('santo', 'VB'), ('ela', 'JJ'), ('dará', 'NN'), ('à', 'NNP'), ('luz', 'VBZ'), ('um', 'JJ'), ('filho', 'NN'), ('e', 'NN'), ('você', 'NN'), ('deverá', 'NN'), ('dar', 'NN'), ('lhe', 'NN'), ('o', 'JJ'), ('nome', 'FW'), ('de', 'FW'), ('jesusb', 'FW'), ('porque', 'NN'), ('ele', 'FW'), ('salvará', 'NN'), ('o', 'NN'), ('seu', 'NN'), ('povo', 'NN'), ('dos', 'NN'), ('seus', 'NN'), ('pecados', 'NN'), ('tudo', 'NN'), ('isso', 'NN'), ('aconteceu', 'NN'), ('para', 'NN'), ('que', 'NN'), ('se', 'FW'), ('cumprisse', 'NN'), ('o', 'NN'), ('que', 'NN'), ('o', 'IN'), ('senhor', 'NN'), ('dissera', 'NN'), ('pelo', 'NN'), ('profeta', 'VBZ'), ('a', 'DT'), ('virgem', 'NN'), ('ficará', 'NN'), ('grávida', 'NN'), ('dará', 'NN'), ('à', 'NNP'), ('luz', 'VBZ'), ('um', 'JJ'), ('filho', 'NN'), ('e', 'NN'), ('lhe', 'NN'), ('chamarão', 'NN'), ('emanuel', 'NN'), ('que', 'NN'), ('significa', 'NN'), ('deus', 'NN'), ('conosco', 'NN'), ('ao', 'IN'), ('acordar', 'NN'), ('josé', 'NN'), ('fez', 'NN'), ('o', 'NN'), ('que', 'NN'), ('o', 'NN'), ('anjo', 'NN'), ('do', 'VBP'), ('senhor', 'VB'), ('lhe', 'VB'), ('tinha', 'JJ'), ('ordenado', 'NN'), ('e', 'NN'), ('recebeu', 'NN'), ('maria', 'NN'), ('como', 'NN'), ('sua', 'NN'), ('esposa', 'NN'), ('mas', 'NN'), ('não', 'JJ'), ('teve', 'NN'), ('relações', 'NN'), ('com', 'NN'), ('ela', 'NN'), ('enquanto', 'IN'), ('ela', 'JJ'), ('não', 'JJ'), ('deu', 'NN'), ('à', 'NNP'), ('luz', 'VBZ'), ('um', 'JJ'), ('filho', 'NN'), ('e', 'NN'), ('ele', 'VBP'), ('lhe', 'JJ'), ('pôs', 'NN'), ('o', 'JJ'), ('nome', 'FW'), ('de', 'FW'), ('jesus', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "data_clean['POS_tokens'] = data_clean['tokens_sentences'].progress_map(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "#print(data_clean['POS_tokens'].head().tolist()[0][:3])\n",
    "#print(data_clean['POS_tokens'].head().tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Klemer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Inspired from https://stackoverflow.com/a/15590384\n",
    "#https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e00b01ab8d24b29816965deb7cea060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=91.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing each word with its POS tag, in each sentence\n",
    "data_clean['tokens_sentences_lemmatized'] = data_clean['POS_tokens'].progress_map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['registro', 'da', 'genealogia', 'de', 'jesus', 'cristo', 'filho', 'de', 'davi', 'filho', 'de', 'abraão', 'abraão', 'gerou', 'isaque', 'isaque', 'gerou', 'jacó', 'jacó', 'gerou', 'judá', 'e', 'seus', 'irmãos', 'judá', 'gerou', 'perez', 'e', 'zerá', 'cuja', 'mãe', 'foi', 'tamar', 'perez', 'gerou', 'esrom', 'esrom', 'gerou', 'arão', 'arão', 'gerou', 'aminadabe', 'aminadabe', 'gerou', 'naassom', 'naassom', 'gerou', 'salmom', 'salmom', 'gerou', 'boaz', 'cuja', 'mãe', 'foi', 'raabe', 'boaz', 'gerou', 'obede', 'cuja', 'mãe', 'foi', 'rute', 'obede', 'gerou', 'jessé', 'e', 'jessé', 'gerou', 'o', 'rei', 'davi', 'davi', 'gerou', 'salomão', 'cuja', 'mãe', 'tinha', 'sido', 'mulher', 'de', 'urias', 'salomão', 'gerou', 'roboão', 'roboão', 'gerou', 'abias', 'abias', 'gerou', 'asa', 'asa', 'gerou', 'josafá', 'josafá', 'gerou', 'jorão', 'jorão', 'gerou', 'uzias', 'uzias', 'gerou', 'jotão', 'jotão', 'gerou', 'acaz', 'acaz', 'gerou', 'ezequias', 'ezequias', 'gerou', 'manassés', 'manassés', 'gerou', 'amom', 'amom', 'gerou', 'josias', 'e', 'josias', 'gerou', 'jeconiasa', 'e', 'seus', 'irmãos', 'no', 'tempo', 'do', 'exílio', 'na', 'babilônia', 'depois', 'do', 'exílio', 'na', 'babilônia', 'jeconias', 'gerou', 'salatiel', 'salatiel', 'gerou', 'zorobabel', 'zorobabel', 'gerou', 'abiúde', 'abiúde', 'gerou', 'eliaquim', 'eliaquim', 'gerou', 'azor', 'azor', 'gerou', 'sadoque', 'sadoque', 'gerou', 'aquim', 'aquim', 'gerou', 'eliúde', 'eliúde', 'gerou', 'eleazar', 'eleazar', 'gerou', 'matã', 'matã', 'gerou', 'jacó', 'e', 'jacó', 'gerou', 'josé', 'marido', 'de', 'maria', 'da', 'qual', 'nasceu', 'jesus', 'que', 'é', 'chamado', 'cristo', 'assim', 'ao', 'todo', 'houve', 'catorze', 'gerações', 'de', 'abraão', 'a', 'davi', 'catorze', 'de', 'davi', 'até', 'o', 'exílio', 'na', 'babilônia', 'e', 'catorze', 'do', 'exílio', 'até', 'o', 'cristo', 'o', 'nascimento', 'de', 'jesus', 'cristo', 'lc', 'foi', 'assim', 'o', 'nascimento', 'de', 'jesus', 'cristo', 'maria', 'sua', 'mãe', 'estava', 'prometida', 'em', 'casamento', 'a', 'josé', 'ma', 'ante', 'que', 'se', 'unissem', 'achou', 'se', 'grávida', 'pelo', 'espírito', 'santo', 'por', 'ser', 'josé', 'seu', 'marido', 'um', 'homem', 'justo', 'e', 'não', 'querendo', 'expô', 'la', 'à', 'desonra', 'pública', 'pretendia', 'anular', 'o', 'casamento', 'secretamente', 'mas', 'depois', 'de', 'ter', 'pensado', 'nisso', 'apareceu', 'lhe', 'um', 'anjo', 'do', 'senhor', 'em', 'sonho', 'e', 'disse', 'josé', 'filho', 'de', 'davi', 'não', 'tema', 'receber', 'maria', 'como', 'sua', 'esposa', 'poi', 'o', 'que', 'nela', 'foi', 'gerado', 'procede', 'do', 'espírito', 'santo', 'ela', 'dará', 'à', 'luz', 'um', 'filho', 'e', 'você', 'deverá', 'dar', 'lhe', 'o', 'nome', 'de', 'jesusb', 'porque', 'ele', 'salvará', 'o', 'seu', 'povo', 'do', 'seus', 'pecados', 'tudo', 'isso', 'aconteceu', 'para', 'que', 'se', 'cumprisse', 'o', 'que', 'o', 'senhor', 'dissera', 'pelo', 'profeta', 'a', 'virgem', 'ficará', 'grávida', 'dará', 'à', 'luz', 'um', 'filho', 'e', 'lhe', 'chamarão', 'emanuel', 'que', 'significa', 'deus', 'conosco', 'ao', 'acordar', 'josé', 'fez', 'o', 'que', 'o', 'anjo', 'do', 'senhor', 'lhe', 'tinha', 'ordenado', 'e', 'recebeu', 'maria', 'como', 'sua', 'esposa', 'ma', 'não', 'teve', 'relações', 'com', 'ela', 'enquanto', 'ela', 'não', 'deu', 'à', 'luz', 'um', 'filho', 'e', 'ele', 'lhe', 'pôs', 'o', 'nome', 'de', 'jesus']]\n"
     ]
    }
   ],
   "source": [
    "print(data_clean['tokens_sentences_lemmatized'].tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regrouping tokens and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Klemer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Klemer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('portuguese')\n",
    "nltk.download('punkt')\n",
    "stopwords= nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords =set(stopwords)\n",
    "stopwords_other = [\"de\",\"a\", \"o\", \"que\",\"e\",\"do\", \"da\", \"em\", \"um\", \"para\", \"é\", \"com\", \"não\", \"uma\", \"os\", \n",
    "                  \"no\", \"se\",\"na\", \"por\",\"mais\", \"as\", \"dos\", \"como\", \"mas\", \"foi\", \"ao\", \"ele\", \"das\", \"tem\",\n",
    "                  \"à\",\"às\",\"seu\", \"sua\",\"ou\",\"ser\", \"quando\", \"muito\", \"há\", \"nos\", \"já\", \"está\", \"eu\", \n",
    "                  \"também\", \"só\",\"pelo\", \"pela\",\"até\", \"isso\" ,\"ela\", \"entre\", \"era\", \"depois\", \"sem\", \"mesmo\", \n",
    "                  \"aos\", \"onde\",\"ter\", \"seus\", \"quem\", \"nas\", \"me\", \"esse\", \"eles\", \"estão\", \n",
    "                  \"você\",\"tinha\", \"foram\", \"essa\",\"num\", \"nem\", \"suas\", \"meu\", \"às\", \"minha\",\"têm\",\n",
    "                  \"numa\", \"pelos\", \"elas\",\"havia\", \"seja\", \"qual\", \"será\", \"nós\", \"tenho\", \n",
    "                  \"lhe\", \"deles\", \"essas\", \"esses\",\"pelas\", \"este\", \"fosse\", \"dele\",\"tu\", \"te\",\n",
    "                  \"vocês\",\"vos\", \"lhes\", \"meus\", \"minhas\", \"teu\",\"tua\", \"teus\", \"tuas\", \"nosso\", \n",
    "                  \"nossa\", \"nossos\",\"nossas\", \"dela\", \"delas\", \"esta\", \"estes\",\n",
    "                  \"estas\", \"aquele\", \"aquela\", \"aqueles\", \"aquelas\", \"isto\", \"aquilo\", \"estou\", \"está\", \n",
    "                  \"estamos\", \"estão\", \"estive\", \"esteve\", \"estivemos\", \"estiveram\", \"estava\", \"estávamos\", \n",
    "                  \"estavam\", \"estivera\", \"estivéramos\", \"esteja\",\"estejamos\", \"estejam\",\"estivesse\", \"estivéssemos\",\n",
    "                  \"estivessem\", \"estiver\", \"estivermos\",\"estiverem\", \"hein\", \"há\", \"havemos\", \"hão\", \"houve\", \n",
    "                  \"houvemos\", \"houveram\", \"houvera\",\"houvéramos\", \"haja\", \"hajamos\", \"hajam\",\"houvesse\", \n",
    "                  \"houvéssemos\",\"houvessem\", \"houver\", \"houvermos\", \"houverem\", \"houverei\", \n",
    "                  \"houverá\", \"houveremos\", \"houverão\", \"houveria\",\"houveríamos\", \"houveriam\" ,\"sou\" ,\"somos\",\n",
    "                  \"são\" ,\"era\", \"éramos\" ,\"eram\", \"fui\", \"foi\", \"fomos\", \"foram\", \"fora\", \"fôramos\", \"seja\", \n",
    "                  \"sejamos\", \"sejam\", \"fosse\", \"fôssemos\", \"fossem\", \"for\" ,\"formos\", \"forem\", \"serei\", \n",
    "                  \"será\", \"seremos\", \"serão\",\"seria\",\"seríamos\" ,\"seriam\" ,\"tenho\",\"tem\",\"temos\",\n",
    "                  \"têm\",\"tinha\" ,\"tínhamos\" ,\"tinham\" ,\"tive\" ,\"teve\" ,\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\n",
    "                  \"tenha\", \"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivessem\",\"tiver\",\"tivermos\",\"tiverem\",\n",
    "                  \"terei\",\"terá\",\"teremos\",\"terão\", \"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\",\"digo\",\n",
    "                   \"sobre\",\"alguém\",'alguma','algo','mais longo','muitos','na maioria das vezes','precisando','mais recente',\n",
    "                   'o mais novo', 'lugar', 'colocar','debaixo','sobre','usar','procurado','seria','não pode','não podia',\n",
    "                   'não fez','não faz','gerar','gerou','geração','registro', 'registrar','porque', 'respondeu', \n",
    "                   'todos', 'mim','ali','lá','onde','lugar','lugares','estava','mc','mt','lc','jo','at','ac']\n",
    "my_stopwords = stopwords_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain # to flatten list of sentences of tokens into list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['tokens'] = data_clean['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "data_clean['tokens'] = data_clean['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genealogia', 'jesus', 'cristo', 'filho', 'davi', 'filho', 'abraão', 'abraão', 'isaque', 'isaque', 'jacó', 'jacó', 'judá', 'irmãos', 'judá', 'perez', 'zerá', 'cuja', 'mãe', 'tamar', 'perez', 'esrom', 'esrom', 'arão', 'arão', 'aminadabe', 'aminadabe', 'naassom', 'naassom', 'salmom', 'salmom', 'boaz', 'cuja', 'mãe', 'raabe', 'boaz', 'obede', 'cuja', 'mãe', 'rute', 'obede', 'jessé', 'jessé', 'rei', 'davi', 'davi', 'salomão', 'cuja', 'mãe', 'sido', 'mulher', 'urias', 'salomão', 'roboão', 'roboão', 'abias', 'abias', 'asa', 'asa', 'josafá', 'josafá', 'jorão', 'jorão', 'uzias', 'uzias', 'jotão', 'jotão', 'acaz', 'acaz', 'ezequias', 'ezequias', 'manassés', 'manassés', 'amom', 'amom', 'josias', 'josias', 'jeconiasa', 'irmãos', 'tempo', 'exílio', 'babilônia', 'exílio', 'babilônia', 'jeconias', 'salatiel', 'salatiel', 'zorobabel', 'zorobabel', 'abiúde']\n"
     ]
    }
   ],
   "source": [
    "print(data_clean['tokens'].tolist()[0][:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nde \\na \\no \\nque \\ne \\ndo \\nda \\nem \\num \\npara \\né \\ncom \\nnão \\numa \\nos \\nno \\nse \\nna \\npor \\nmais \\nas \\ndos \\ncomo \\nmas \\nfoi \\nao \\nele \\ndas \\ntem \\nà \\nseu \\nsua \\nou \\nser \\nquando \\nmuito \\nhá \\nnos \\njá \\nestá \\neu \\ntambém \\nsó \\npelo \\npela \\naté \\nisso \\nela \\nentre \\nera \\ndepois \\nsem \\nmesmo \\naos \\nter \\nseus \\nquem \\nnas \\nme \\nesse \\neles \\nestão \\nvocê \\ntinha \\nforam \\nessa \\nnum \\nnem \\nsuas \\nmeu \\nàs \\nminha \\ntêm \\nnuma \\npelos \\nelas \\nhavia \\nseja \\nqual \\nserá \\nnós \\ntenho \\nlhe \\ndeles \\nessas \\nesses \\npelas \\neste \\nfosse \\ndele \\ntu \\nte \\nvocês \\nvos \\nlhes \\nmeus \\nminhas\\nteu \\ntua\\nteus\\ntuas\\nnosso \\nnossa\\nnossos\\nnossas\\ndela \\ndelas \\nesta \\nestes \\nestas \\naquele \\naquela \\naqueles \\naquelas \\nisto \\naquilo \\nestou\\nestá\\nestamos\\nestão\\nestive\\nesteve\\nestivemos\\nestiveram\\nestava\\nestávamos\\nestavam\\nestivera\\nestivéramos\\nesteja\\nestejamos\\nestejam\\nestivesse\\nestivéssemos\\nestivessem\\nestiver\\nestivermos\\nestiverem\\nhei\\nhá\\nhavemos\\nhão\\nhouve\\nhouvemos\\nhouveram\\nhouvera\\nhouvéramos\\nhaja\\nhajamos\\nhajam\\nhouvesse\\nhouvéssemos\\nhouvessem\\nhouver\\nhouvermos\\nhouverem\\nhouverei\\nhouverá\\nhouveremos\\nhouverão\\nhouveria\\nhouveríamos\\nhouveriam\\nsou\\nsomos\\nsão\\nera\\néramos\\neram\\nfui\\nfoi\\nfomos\\nforam\\nfora\\nfôramos\\nseja\\nsejamos\\nsejam\\nfosse\\nfôssemos\\nfossem\\nfor\\nformos\\nforem\\nserei\\nserá\\nseremos\\nserão\\nseria\\nseríamos\\nseriam\\ntenho\\ntem\\ntemos\\ntém\\ntinha\\ntínhamos\\ntinham\\ntive\\nteve\\ntivemos\\ntiveram\\ntivera\\ntivéramos\\ntenha\\ntenhamos\\ntenham\\ntivesse\\ntivéssemos\\ntivessem\\ntiver\\ntivermos\\ntiverem\\nterei\\nterá\\nteremos\\nterão\\nteria\\nteríamos\\nteriam\\nperguntar\\ndisse\\nlhes\\ndigo-vos\\ndizer\\nhavia\\nalguém\\nalgo\\nalguma\\n\".split())\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# coding: utf8\n",
    "from __future__ import unicode_literals\n",
    "\n",
    " \n",
    "stop_words = set(\"\"\" \n",
    "\"\"\" \n",
    "de \n",
    "a \n",
    "o \n",
    "que \n",
    "e \n",
    "do \n",
    "da \n",
    "em \n",
    "um \n",
    "para \n",
    "é \n",
    "com \n",
    "não \n",
    "uma \n",
    "os \n",
    "no \n",
    "se \n",
    "na \n",
    "por \n",
    "mais \n",
    "as \n",
    "dos \n",
    "como \n",
    "mas \n",
    "foi \n",
    "ao \n",
    "ele \n",
    "das \n",
    "tem \n",
    "à \n",
    "seu \n",
    "sua \n",
    "ou \n",
    "ser \n",
    "quando \n",
    "muito \n",
    "há \n",
    "nos \n",
    "já \n",
    "está \n",
    "eu \n",
    "também \n",
    "só \n",
    "pelo \n",
    "pela \n",
    "até \n",
    "isso \n",
    "ela \n",
    "entre \n",
    "era \n",
    "depois \n",
    "sem \n",
    "mesmo \n",
    "aos \n",
    "ter \n",
    "seus \n",
    "quem \n",
    "nas \n",
    "me \n",
    "esse \n",
    "eles \n",
    "estão \n",
    "você \n",
    "tinha \n",
    "foram \n",
    "essa \n",
    "num \n",
    "nem \n",
    "suas \n",
    "meu \n",
    "às \n",
    "minha \n",
    "têm \n",
    "numa \n",
    "pelos \n",
    "elas \n",
    "havia \n",
    "seja \n",
    "qual \n",
    "será \n",
    "nós \n",
    "tenho \n",
    "lhe \n",
    "deles \n",
    "essas \n",
    "esses \n",
    "pelas \n",
    "este \n",
    "fosse \n",
    "dele \n",
    "tu \n",
    "te \n",
    "vocês \n",
    "vos \n",
    "lhes \n",
    "meus \n",
    "minhas\n",
    "teu \n",
    "tua\n",
    "teus\n",
    "tuas\n",
    "nosso \n",
    "nossa\n",
    "nossos\n",
    "nossas\n",
    "dela \n",
    "delas \n",
    "esta \n",
    "estes \n",
    "estas \n",
    "aquele \n",
    "aquela \n",
    "aqueles \n",
    "aquelas \n",
    "isto \n",
    "aquilo \n",
    "estou\n",
    "está\n",
    "estamos\n",
    "estão\n",
    "estive\n",
    "esteve\n",
    "estivemos\n",
    "estiveram\n",
    "estava\n",
    "estávamos\n",
    "estavam\n",
    "estivera\n",
    "estivéramos\n",
    "esteja\n",
    "estejamos\n",
    "estejam\n",
    "estivesse\n",
    "estivéssemos\n",
    "estivessem\n",
    "estiver\n",
    "estivermos\n",
    "estiverem\n",
    "hei\n",
    "há\n",
    "havemos\n",
    "hão\n",
    "houve\n",
    "houvemos\n",
    "houveram\n",
    "houvera\n",
    "houvéramos\n",
    "haja\n",
    "hajamos\n",
    "hajam\n",
    "houvesse\n",
    "houvéssemos\n",
    "houvessem\n",
    "houver\n",
    "houvermos\n",
    "houverem\n",
    "houverei\n",
    "houverá\n",
    "houveremos\n",
    "houverão\n",
    "houveria\n",
    "houveríamos\n",
    "houveriam\n",
    "sou\n",
    "somos\n",
    "são\n",
    "era\n",
    "éramos\n",
    "eram\n",
    "fui\n",
    "foi\n",
    "fomos\n",
    "foram\n",
    "fora\n",
    "fôramos\n",
    "seja\n",
    "sejamos\n",
    "sejam\n",
    "fosse\n",
    "fôssemos\n",
    "fossem\n",
    "for\n",
    "formos\n",
    "forem\n",
    "serei\n",
    "será\n",
    "seremos\n",
    "serão\n",
    "seria\n",
    "seríamos\n",
    "seriam\n",
    "tenho\n",
    "tem\n",
    "temos\n",
    "tém\n",
    "tinha\n",
    "tínhamos\n",
    "tinham\n",
    "tive\n",
    "teve\n",
    "tivemos\n",
    "tiveram\n",
    "tivera\n",
    "tivéramos\n",
    "tenha\n",
    "tenhamos\n",
    "tenham\n",
    "tivesse\n",
    "tivéssemos\n",
    "tivessem\n",
    "tiver\n",
    "tivermos\n",
    "tiverem\n",
    "terei\n",
    "terá\n",
    "teremos\n",
    "terão\n",
    "teria\n",
    "teríamos\n",
    "teriam\n",
    "perguntar\n",
    "disse\n",
    "lhes\n",
    "digo-vos\n",
    "dizer\n",
    "havia\n",
    "alguém\n",
    "algo\n",
    "alguma\n",
    "\"\"\" \"\"\"\".split())\n",
    "\"\"\"\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nnlp = spacy.load(\"pt_core_news_sm\")\\n\\nstopwords=STOP_WORDS\\n\\ntexts = data_clean[\\'texts\\'].tolist()\\ndoc = list(nlp.pipe(texts))\\n\\nnlp = spacy.load(\"pt\", disable=[\\'parser\\', \\'tagger\\', \\'ner\\'])\\nstops = stopwords.words(\"portuguese\")\\n\\ndef normalize(comment, lowercase, remove_stopwords):\\n    if lowercase:\\n        comment = comment.lower()\\n    comment = nlp(comment)\\n    lemmatized = list()\\n    for word in comment:\\n        lemma = word.lemma_.strip()\\n        if lemma:\\n            if not remove_stopwords or (remove_stopwords and lemma not in stops):\\n                lemmatized.append(lemma)\\n    return \" \".join(lemmatized)\\n\\n\\ndata_df[\\'text_after_clean\\'] = data_clean[\\'texts\\'].apply(normalize, lowercase=True, remove_stopwords=True)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "stopwords=STOP_WORDS\n",
    "\n",
    "texts = data_clean['texts'].tolist()\n",
    "doc = list(nlp.pipe(texts))\n",
    "\n",
    "nlp = spacy.load(\"pt\", disable=['parser', 'tagger', 'ner'])\n",
    "stops = stopwords.words(\"portuguese\")\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "data_df['text_after_clean'] = data_clean['texts'].apply(normalize, lowercase=True, remove_stopwords=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORDCLOUD EXPLORATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-bb0466ad336c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Import the wordcloud library\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Concatenar as palavras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "#Exploratory Analysis\n",
    "# Import the wordcloud library\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "\n",
    "#Concatenar as palavras\n",
    "data_words = \" \".join(list(data_clean['texts'].values)) #tutorial 1 \n",
    "\n",
    "#stopwords=nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "stop_words= set(STOPWORDS) ##Definindo uma lista de STOPWORDS  Tutorial 6\n",
    "stop_words= my_stopwords\n",
    "\n",
    "# Criar uma WordCloud # lower max_font_size\n",
    "wordcloud= WordCloud(stopwords= my_stopwords,background_color=\"black\",  \n",
    "                     contour_width=3,contour_color=\"steelblue\",\n",
    "                     max_words=50000,random_state=42, width=1600, height=800,max_font_size=40).generate(data_words)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "#wordcloud.to_file(\"WorldCloud_lda_gospels.png\") #Salva quando tiver certeza que as imagens estão certas. \n",
    "\n",
    "\n",
    "#Gerar a WordCloud\n",
    " #for index, doc in enumerate(data_clean.columns):\n",
    "\n",
    "\n",
    "# Visualizar as WorldCloud\n",
    "# Mostrar a imagem final \n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.title(\"Nuvem de palavras dos Evangelhos\", fontsize=40, color=\"red\")\n",
    "#fig, ax=plt.subplots(figsize=(16,6))\n",
    "#ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#ax.set_axis_off()\n",
    "#wordcloud.to_image();\n",
    "#wordcloud.to_file(\"WorldCloud_lda_gospels.png\") #Salva quando tiver certeza que as imagens estão certas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation**\n",
    "\n",
    "*Prepare bi-grams and tri-grams*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-336a54a814d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Phrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-213-c06f2bdbcbb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbigram_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrigram_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigram_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbigram_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Phrases' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = data_clean['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare objects for LDA gensim implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-200-0b009fd6379b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpora' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-201-06f3b7cb86bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary_LDA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdictionary_LDA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary_LDA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpora' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Running LDA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-202-e9da63a94a3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(123456)\n",
    "num_topics = 30\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick exploration of LDA results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-1e86c8a18103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\": \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Allocating topics to documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   não julguem para que vocês não sejam julgados    pois da mesma forma que julgarem vocês serão julgados e a medida que usarem também será usada para medir vocês     por que você repara no cisco que está no olho do seu irmão e não se dá conta da viga que está em seu próprio olho    como você pode dizer ao seu irmão   deixe me tirar o cisco do seu olho  quando há uma viga no seu    hipócrita tire primeiro a viga do seu olho e então você verá claramente para tirar o cisco do olho do seu irmão    \n"
     ]
    }
   ],
   "source": [
    "print(data_clean.texts.loc[6][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-211-4a57ce702cc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Exibir o lDA MODEL na posiçao desejada\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "#Exibir o lDA MODEL na posiçao desejada\n",
    "lda_model[corpus[6]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predicting topics on unseen documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-68dc56a3945c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'''não julguem para que vocês não sejam julgados    pois da mesma forma que julgarem vocês serão julgados e a medida que usarem também será usada para medir vocês     por que você repara no cisco que está no olho do seu irmão e não se dá conta da viga que está em seu próprio olho    como você pode dizer ao seu irmão   deixe me tirar o cisco do seu olho  quando há uma viga no seu    hipócrita tire primeiro a viga do seu olho e então você verá claramente para tirar o cisco do olho do seu irmão   '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdictionary_LDA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic #'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'words in topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "document = '''não julguem para que vocês não sejam julgados    pois da mesma forma que julgarem vocês serão julgados e a medida que usarem também será usada para medir vocês     por que você repara no cisco que está no olho do seu irmão e não se dá conta da viga que está em seu próprio olho    como você pode dizer ao seu irmão   deixe me tirar o cisco do seu olho  quando há uma viga no seu    hipócrita tire primeiro a viga do seu olho e então você verá claramente para tirar o cisco do olho do seu irmão   '''\n",
    "tokens = word_tokenize(document)\n",
    "topics = lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20)\n",
    "pd.DataFrame([(el[0], round(el[1],2), topics[el[0]][1]) for el in lda_model[dictionary_LDA.doc2bow(tokens)]], columns=['topic #', 'weight', 'words in topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced exploration of LDA results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Allocation of topics in all documents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [lda_model[corpus[i]] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "    res = pd.DataFrame(columns=range(num_topics))\n",
    "    for topic_weight in topics_document:\n",
    "        res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "    return res\n",
    "\n",
    "topics_document_to_dataframe([(9, 0.03853655432967504), (15, 0.09130117862212643), (18, 0.8692868808484044)], 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-216-ce1083c907df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Like TF-IDF, create a matrix of topic weighting, with documents as rows and topics as columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdocument_topic\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopics_document_to_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_document\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopics_document\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topics' is not defined"
     ]
    }
   ],
   "source": [
    "# Like TF-IDF, create a matrix of topic weighting, with documents as rows and topics as columns\n",
    "document_topic = \\\n",
    "pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in topics]) \\\n",
    "  .reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_topic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-217-bc8dfbf106de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdocument_topic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'document_topic' is not defined"
     ]
    }
   ],
   "source": [
    "document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_topic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-51b53ef861c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Which document are about topic 14\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdocument_topic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'document_topic' is not defined"
     ]
    }
   ],
   "source": [
    "# Which document are about topic 14\n",
    "document_topic.sort_values(14, ascending=False)[14].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.articles.loc[91][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the distribution of topics in all documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import seaborn as sns; sns.set(rc={'figure.figsize':(10,20)})\n",
    "#sns.heatmap(document_topic.loc[document_topic.idxmax(axis=1).sort_values().index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(rc={'figure.figsize':(10,5)})\n",
    "#document_topic.idxmax(axis=1).value_counts().plot.bar(color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here a short legend to explain the vis:\n",
    "# size of bubble: proportional to the proportions of the topics across the N total tokens in the corpus\n",
    "# red bars: estimated number of times a given term was generated by a given topic\n",
    "# blue bars: overall frequency of each term in the corpus\n",
    "# -- Relevance of words is computed with a parameter lambda\n",
    "# -- Lambda optimal value ~0.6 (https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf)\n",
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA com SKlEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LDA model training and results visualization - tutorial 1 \n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 30\n",
    "number_words = 20\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing LDA model results -- 8-https://medium.com/somos-tera/como-modelar-topicos-atraves-de-latent-dirichlet-allocation-lda-atraves-da-biblioteca-gensim-1fa17357ad4b\n",
    "%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "#iit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if (1 == 1):\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
