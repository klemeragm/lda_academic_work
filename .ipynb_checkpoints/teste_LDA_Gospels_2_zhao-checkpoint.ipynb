{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## 1 https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0 \n",
    "##Auctor: Shashank Kapadia -- 2019  (Testa esse aqui tbm)\n",
    "\n",
    "## 2 https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html -- doc\n",
    "\n",
    "##3 https://sigmoidal.ai/como-criar-uma-wordcloud-em-python/  -- WorldClouds ## \n",
    "##4 http://tirandolicoesdetudo.com.br/criando-uma-nuvem-de-palavras-wordcloud-com-dados-do-meu-cv/ -- WorldCloud ##\n",
    "\n",
    "##5 https://www.youtube.com/watch?v=iQ1bfDMCv_c&list=PLf6b7z7NwdGVMXPEvoJu64jxtE58q4CH8&index=3&t=9s - Videos da ALICE ZHAO##\n",
    "#https://github.com/adashofdata/nlp-in-python-tutorial - 2018\n",
    "\n",
    "\n",
    "## 6 https://gist.github.com/alopes/5358189 - lista de StopWords em português com correspondências. \n",
    "\n",
    "\n",
    "## 7 https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e\n",
    "\n",
    "##8 - https://medium.com/somos-tera/como-modelar-t%C3%B3picos-atrav%C3%A9s-de-latent-dirichlet-allocation-lda-atrav%C3%A9s-da-biblioteca-gensim-1fa17357ad4b -- 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3 and python 2 #para puxar as bibliotecas que estejam disponíveis na versões. \n",
    "from __future__ import print_function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap</th>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A Genealogia de Jesus 1 Registro da genealogia...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Visita dos Magos 1 Depois que Jesus nasceu e...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>João Batista Prepara o Caminho (Mc 1.2-8 Lc 3....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A Tentação de Jesus (Mc 1.1213 Lc 4.1-13) 1 En...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>As Bem-aventuranças (Lc 6.20-23)\\r\\n1 Vendo as...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>A Ajuda aos Necessitados 1 “Tenham o cuidado d...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>“Não julguem, para que vocês não sejam julgado...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>A Cura de um Leproso Quando ele desceu do mont...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Jesus Cura um Paralítico Entrando Jesus num ba...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Jesus Envia os Doze Chamando seus doze discípu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cap                                                doc    \n",
       "0   1  A Genealogia de Jesus 1 Registro da genealogia... NaN\n",
       "1   2  A Visita dos Magos 1 Depois que Jesus nasceu e... NaN\n",
       "2   3  João Batista Prepara o Caminho (Mc 1.2-8 Lc 3.... NaN\n",
       "3   4  A Tentação de Jesus (Mc 1.1213 Lc 4.1-13) 1 En... NaN\n",
       "4   5  As Bem-aventuranças (Lc 6.20-23)\\r\\n1 Vendo as... NaN\n",
       "5   6  A Ajuda aos Necessitados 1 “Tenham o cuidado d... NaN\n",
       "6   7  “Não julguem, para que vocês não sejam julgado... NaN\n",
       "7   8  A Cura de um Leproso Quando ele desceu do mont... NaN\n",
       "8   9  Jesus Cura um Paralítico Entrando Jesus num ba... NaN\n",
       "9  10  Jesus Envia os Doze Chamando seus doze discípu... NaN"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ler os dados aqui. \n",
    "data_df = pd.read_csv('docs_gospels_Mateus_2.csv', sep=';', encoding='utf-8')\n",
    "#Imprima as linhas com .head()\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef combined_text(docs):\\n    combined_text = ' '.join(docs)\\n    return combined_text\\n\\n##Dados combinados dentro da coluna\\ndocs = {key: [combine_text(value)] for (key,value) in docs.items}\\n\\npd.set_option('max_colwidth',150)\\n\\ndocs= pd.DataFrame.from_dict(data_combined).transpose()\\ndocs.columns = ['doc']\\ndocs = docs.sort_index()\\ndocs \\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def combined_text(docs):\n",
    "    combined_text = ' '.join(docs)\n",
    "    return combined_text\n",
    "\n",
    "##Dados combinados dentro da coluna\n",
    "docs = {key: [combine_text(value)] for (key,value) in docs.items}\n",
    "\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "docs= pd.DataFrame.from_dict(data_combined).transpose()\n",
    "docs.columns = ['doc']\n",
    "docs = docs.sort_index()\n",
    "docs \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Genealogia de Jesus 1 Registro da genealogia...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Visita dos Magos 1 Depois que Jesus nasceu e...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>João Batista Prepara o Caminho (Mc 1.2-8 Lc 3....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Tentação de Jesus (Mc 1.1213 Lc 4.1-13) 1 En...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As Bem-aventuranças (Lc 6.20-23)\\r\\n1 Vendo as...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc    \n",
       "0  A Genealogia de Jesus 1 Registro da genealogia... NaN\n",
       "1  A Visita dos Magos 1 Depois que Jesus nasceu e... NaN\n",
       "2  João Batista Prepara o Caminho (Mc 1.2-8 Lc 3.... NaN\n",
       "3  A Tentação de Jesus (Mc 1.1213 Lc 4.1-13) 1 En... NaN\n",
       "4  As Bem-aventuranças (Lc 6.20-23)\\r\\n1 Vendo as... NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removendo a coluna de capítulos, pois não precisamos das mesmas. \n",
    "data_df = data_df.drop('cap', axis=1) #df.drop('column_name', axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "#next(iter(data_df.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "#next(iter(data_df.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ncombined_text=0\\ndef combine_text(doc):\\n   '''Takes a list of text and combines them into one large chunk of text.'''\\n    combined_text = ' '.join(doc)\\n    return combined_text\\n\\nprint(combined_text)\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "combined_text=0\n",
    "def combine_text(doc):\n",
    "   '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(doc)\n",
    "    return combined_text\n",
    "\n",
    "print(combined_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation tutorial 5\n",
    "import re\n",
    "import string \n",
    "\n",
    "def clean_text_docs_round1(doc):\n",
    "        doc= doc.lower()\n",
    "        doc= re.sub('[.*-,/?!:;()\\']', '',doc)\n",
    "        doc= re.sub('[-|0-9]', '', doc)\n",
    "        doc= re.sub('[%s]' % re.escape(string.punctuation), '', doc)\n",
    "        doc= re.sub('\\w*\\d\\w*', '',doc)\n",
    "        return doc\n",
    "    \n",
    "round1 = lambda x: clean_text_docs_round1(x)\n",
    "    \n",
    "# Convert the titles to lowercase\n",
    "#docs['doc'] = docs['doc'].map(lambda x: x.lower())\n",
    "#docs['doc'] = docs['doc'].map(lambda x: re.sub('[,\\.!?^~´`0:9]', ' ', x))\n",
    "# Print out the first rows of papers\n",
    "#docs['doc'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-8a13434b6d4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Atualizar documento com o texto atualizado fase 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-93a00fbfcad1>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mround1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text_docs_round1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Convert the titles to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-93a00fbfcad1>\u001b[0m in \u001b[0;36mclean_text_docs_round1\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean_text_docs_round1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[.*-,/?!:;()\\']'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[-|0-9]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#Atualizar documento com o texto atualizado fase 1\n",
    "data_clean = pd.DataFrame(data_df.doc.apply(round1))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(doc):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    doc = re.sub('[.*‘’“”]', '', doc)\n",
    "    doc = re.sub('\\r', '', doc)\n",
    "    doc = re.sub('\\n', '', doc)\n",
    "    return doc\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-0af10ca35d58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let's take a look at the updated text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean.doc.apply(round2))\n",
    "data_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amazenar os dados em um formato pickle para ulilizado posteriormente\n",
    "#import pickle\n",
    "#data_clean.to_pickle('dados_limpos.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lembrar de ver se é possivel pegar os capítulos e fazer a document_term_ matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando as NLTK libraries\n",
    "#import nltk\n",
    "#nltk.download('portuguese') #Biblioteca não está reconhecendo valores em porturguês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-773018166172>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Concatenar as palavras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdata_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'doc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#tutorial 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#stopwords=nltk.corpus.stopwords.words('portuguese')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "#Exploratory Analysis\n",
    "# Import the wordcloud library\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "\n",
    "#Concatenar as palavras\n",
    "data_words = ','.join(list(data_clean['doc'].values)) #tutorial 1 \n",
    "\n",
    "#stopwords=nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "\n",
    "stop_words= set(STOPWORDS) ##Definindo uma lista de STOPWORDS  Tutorial 6\n",
    "stopwords.update([\"de\",\"a\", \"o\", \"que\",\"e\",\"do\", \"da\", \"em\", \"um\", \"para\", \"é\", \"com\", \"não\", \"uma\", \"os\", \n",
    "                  \"no\", \"se\",\"na\", \"por\",\"mais\", \"as\", \"dos\", \"como\", \"mas\", \"foi\", \"ao\", \"ele\", \"das\", \"tem\",\n",
    "                  \"à\",\"às\",\"seu\", \"sua\",\"ou\",\"ser\", \"quando\", \"muito\", \"há\", \"nos\", \"já\", \"está\", \"eu\", \n",
    "                  \"também\", \"só\",\"pelo\", \"pela\",\"até\", \"isso\" ,\"ela\", \"entre\", \"era\", \"depois\", \"sem\", \"mesmo\", \n",
    "                  \"aos\", \"onde\",\"ter\", \"seus\", \"quem\", \"nas\", \"me\", \"esse\", \"eles\", \"estão\", \n",
    "                  \"você\",\"tinha\", \"foram\", \"essa\",\"num\", \"nem\", \"suas\", \"meu\", \"às\", \"minha\",\"têm\",\n",
    "                  \"numa\", \"pelos\", \"elas\",\"havia\", \"seja\", \"qual\", \"será\", \"nós\", \"tenho\", \n",
    "                  \"lhe\", \"deles\", \"essas\", \"esses\",\"pelas\", \"este\", \"fosse\", \"dele\",\"tu\", \"te\",\n",
    "                  \"vocês\",\"vos\", \"lhes\", \"meus\", \"minhas\", \"teu\",\"tua\", \"teus\", \"tuas\", \"nosso\", \n",
    "                  \"nossa\", \"nossos\",\"nossas\", \"dela\", \"delas\", \"esta\", \"estes\",\n",
    "                  \"estas\", \"aquele\", \"aquela\", \"aqueles\", \"aquelas\", \"isto\", \"aquilo\", \"estou\", \"está\", \n",
    "                  \"estamos\", \"estão\", \"estive\", \"esteve\", \"estivemos\", \"estiveram\", \"estava\", \"estávamos\", \n",
    "                  \"estavam\", \"estivera\", \"estivéramos\", \"esteja\",\"estejamos\", \"estejam\",\"estivesse\", \"estivéssemos\",\n",
    "                  \"estivessem\", \"estiver\", \"estivermos\",\"estiverem\", \"hein\", \"há\", \"havemos\", \"hão\", \"houve\", \n",
    "                  \"houvemos\", \"houveram\", \"houvera\",\"houvéramos\", \"haja\", \"hajamos\", \"hajam\",\"houvesse\", \n",
    "                  \"houvéssemos\",\"houvessem\", \"houver\", \"houvermos\", \"houverem\", \"houverei\", \n",
    "                  \"houverá\", \"houveremos\", \"houverão\", \"houveria\",\"houveríamos\", \"houveriam\" ,\"sou\" ,\"somos\",\n",
    "                  \"são\" ,\"era\", \"éramos\" ,\"eram\", \"fui\", \"foi\", \"fomos\", \"foram\", \"fora\", \"fôramos\", \"seja\", \n",
    "                  \"sejamos\", \"sejam\", \"fosse\", \"fôssemos\", \"fossem\", \"for\" ,\"formos\", \"forem\", \"serei\", \n",
    "                  \"será\", \"seremos\", \"serão\",\"seria\",\"seríamos\" ,\"seriam\" ,\"tenho\",\"tem\",\"temos\",\n",
    "                  \"têm\",\"tinha\" ,\"tínhamos\" ,\"tinham\" ,\"tive\" ,\"teve\" ,\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\n",
    "                  \"tenha\", \"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivessem\",\"tiver\",\"tivermos\",\"tiverem\",\n",
    "                  \"terei\",\"terá\",\"teremos\",\"terão\", \"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\"])\n",
    "\n",
    "# Criar uma WordCloud\n",
    "wordcloud= WordCloud(stopwords= stop_words,background_color=\"white\",  \n",
    "                     contour_width=3,contour_color=\"steelblue\",\n",
    "                     max_words=50000,random_state=42, width=1600, height=800)\n",
    "\n",
    "#Gerar a WordCloud\n",
    " #for index, doc in enumerate(data_clean.columns):\n",
    "wordcloud.generate(data_words)\n",
    "\n",
    "# Visualizar as WorldCloud\n",
    "# Mostrar a imagem final \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Nuvem de palavras dos Evangelhos\", fontsize=40, color=\"red\")\n",
    "fig, ax=plt.subplots(figsize=(16,6))\n",
    "ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "ax.set_axis_off()\n",
    "wordcloud.to_image();\n",
    "\n",
    "#wordcloud.to_file(\"WorldCloud_lda_gospels.png\") #Salva quando tiver certeza que as imagens estão certas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare text for LDA Analysis\n",
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def plot_30_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:30]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='30 palavras mais comuns')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "    #plt.savefig('30_palavras_mais_comuns.png', format='png')\n",
    "\n",
    "\n",
    "\n",
    "stop_words=set(STOPWORDS)\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = [(\"de\",\"a\",\"o\",\"que\",\"e\",\"do\", \"da\", \"em\", \"um\", \"para\", \"é\", \"com\", \"não\", \"uma\", \"os\", \"no\", \"se\", \"na\", \"por\", \n",
    "                  \"mais\", \"as\", \"dos\", \"como\", \"mas\", \"foi\", \"ao\", \"ele\", \"das\", \"tem\", \"à\",\"às\",\"seu\", \"sua\",\"ou\", \n",
    "                  \"ser\", \"quando\", \"muito\", \"há\", \"nos\", \"já\", \"está\", \"eu\", \"também\", \"só\", \"pelo\", \"pela\",  \n",
    "                  \"até\", \"isso\" ,\"ela\", \"entre\", \"era\", \"depois\", \"sem\", \"mesmo\", \"aos\", \"onde\",\"ter\", \"seus\", \"quem\", \"nas\", \"me\", \"esse\", \"eles\", \"estão\", \"você\",\"tinha\", \"foram\", \"essa\", \n",
    "                  \"num\", \"nem\", \"suas\", \"meu\", \"às\", \"minha\",\"têm\", \"numa\", \"pelos\", \"elas\",\"havia\", \"seja\", \"qual\", \"será\", \"nós\", \"tenho\", \"lhe\", \"deles\", \"essas\", \"esses\", \"pelas\", \"este\", \"fosse\", \"dele\", \n",
    "                  \"tu\", \"te\",\"vocês\",\"vos\", \"lhes\", \"meus\", \"minhas\", \"teu\", \"tua\", \"teus\", \"tuas\", \"nosso\", \"nossa\", \"nossos\",\"nossas\",\n",
    "                  \"dela\", \"delas\", \"esta\", \"estes\", \"estas\", \"aquele\", \"aquela\", \"aqueles\", \"aquelas\", \"isto\", \"aquilo\", \"estou\", \"está\", \n",
    "                  \"estamos\", \"estão\", \"estive\", \"esteve\", \"estivemos\", \"estiveram\", \"estava\", \"estávamos\", \"estavam\", \"estivera\", \"estivéramos\", \"esteja\",\n",
    "                  \"estejamos\", \"estejam\",\"estivesse\", \"estivéssemos\",\"estivessem\", \"estiver\", \"estivermos\", \n",
    "                  \"estiverem\", \"hein\", \"há\", \"havemos\", \"hão\", \"houve\", \"houvemos\", \"houveram\", \"houvera\", \n",
    "                  \"houvéramos\", \"haja\", \"hajamos\", \"hajam\",\"houvesse\", \"houvéssemos\", \"houvessem\", \"houver\",\n",
    "                   \"houvermos\", \"houverem\", \"houverei\", \"houverá\", \"houveremos\", \"houverão\", \"houveria\",\"houveríamos\", \"houveriam\" ,\"sou\" ,\"somos\" ,\"são\" ,\"era\", \"éramos\" ,\"eram\", \"fui\", \"foi\", \"fomos\", \"foram\", \"fora\", \"fôramos\", \"seja\", \n",
    "                  \"sejamos\", \"sejam\", \"fosse\", \"fôssemos\", \"fossem\", \"for\",\"formos\", \"forem\", \"serei\",\"será\", \"seremos\", \n",
    "                  \"serão\",\"seria\",\"seríamos\" ,\"seriam\" ,\"tenho\",\"tem\",\"temos\",\n",
    "                  \"têm\",\"tinha\" ,\"tínhamos\" ,\"tinham\" ,\"tive\" ,\"teve\" ,\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\n",
    "                  \"tenha\", \"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivessem\",\"tiver\",\"tivermos\",\"tiverem\",\n",
    "                  \"terei\",\"terá\",\"teremos\",\"terão\",\"teria\",\"teríamos\",\"teriam\",\"então\",\"assim\", \"pois\")]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "\n",
    "#Adicionando novas Stop-Words\n",
    "count_vectorizer= CountVectorizer(stop_words = stop_words)\n",
    "\n",
    "#Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(data_clean['doc'])\n",
    "\n",
    "# Visualize the 30 most common words\n",
    "plot_30_most_common_words(count_data, count_vectorizer) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-abefd129f33d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Create and fit the LDA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# Print the topics found by the LDA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Topics found via LDA:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count_data' is not defined"
     ]
    }
   ],
   "source": [
    "#LDA model training and results visualization - tutorial 1 \n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 9\n",
    "number_words = 20\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing LDA model results -- 8-https://medium.com/somos-tera/como-modelar-topicos-atraves-de-latent-dirichlet-allocation-lda-atraves-da-biblioteca-gensim-1fa17357ad4b\n",
    "%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "#iit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if (1 == 1):\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
